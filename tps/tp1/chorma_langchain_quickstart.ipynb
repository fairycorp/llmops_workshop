{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chroma & Lanchain quickstart \n",
    "\n",
    "In this notebook we will try to see how to interact with chomadb and langchain in order to wrap all this into a script for the next activities ü§ó\n",
    "\n",
    "## Chroma docker container \n",
    "\n",
    "First you've must run chroma inside docker with the command below : \n",
    "\n",
    "```bash\n",
    "docker run -d \\\n",
    "  --name chroma-db \\\n",
    "  -p 8001:8001 \\\n",
    "  -e ALLOW_RESET=true \\\n",
    "  -e ANONYMIZED_TELEMETRY=false \\\n",
    "  -e CHROMA_SERVER_AUTH_CREDENTIALS_ENABLE=false \\\n",
    "  -e CHROMA_SERVER_HTTP_PORT=8000 \\\n",
    "  -v \"$(pwd)/data/chroma:/chroma/chroma\" \\\n",
    "  --network=host \\\n",
    "  ghcr.io/chroma-core/chroma:latest\n",
    "```\n",
    "\n",
    "You should have a chroma container running, you can check it with de `docker ps` command. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from chromadb.config import Settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with no authentication on the port of your choice \n",
    "chroma_client = chromadb.HttpClient(host='localhost', \n",
    "                                    port=8001,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the [official quickstart](https://docs.trychroma.com/docs/overview/getting-started) to create a collection named `document_store`, add document into it and try to query the collection like in the cells below \n",
    "\n",
    "> ‚ö†Ô∏è you will not have the same values as output since we use differents data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.count_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['document_store',\n",
       " 'index_b0bc2f8a-da73-4b6f-a2f7-03dd57a644d4',\n",
       " 'index_601514aa-c91f-4c50-98ce-bf098a471754',\n",
       " 'index_default',\n",
       " 'index_172ac334-b578-472f-b888-79b4d08eb6aa',\n",
       " 'index_9885888a-e4e9-41e6-abf3-01e17be7452b']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = chroma_client.get_collection(name='document_store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0e3f30ea-574c-4134-a103-0b2669491080',\n",
       " '6f4e7096-7afd-4790-a550-d47862292341',\n",
       " 'cc81e872-194a-4780-b237-334fea3b3a85',\n",
       " '81d4d14a-9db2-4b3c-af7e-212aba06a779',\n",
       " '828b3041-f982-4594-8106-449d791fb4b4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection.get()['ids'][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain in a nutshell \n",
    "\n",
    "[LangChain](https://python.langchain.com/v0.1/docs/get_started/quickstart/) is a framework that simplifies building applications powered by language models, providing components for document handling, memory, agents, and chains to orchestrate complex workflows. \n",
    "\n",
    "Its primary purpose is to help developers create context-aware AI applications that can connect language models to external data sources and tools.RetryClaude can make mistakes. Please double-check responses. Let's take a look on how it's work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"1. Practice regularly: Coding is a skill that improves with practice. Set aside time each day to work on coding challenges, projects, or practice problems to improve your coding skills.\\n\\n2. Focus on problem-solving: Coding is essentially problem-solving, so focus on understanding the problem at hand before writing any code. Break the problem down into smaller, manageable tasks and work through them sequentially.\\n\\n3. Collaborate with others: Join coding communities, attend coding meetups, or work on projects with other developers. Collaborating with others can help you learn new techniques, get feedback on your code, and discover best practices.\\n\\n4. Learn from your mistakes: When you encounter bugs or errors in your code, take the time to understand what went wrong and how you can improve it. Learning from your mistakes can help you become a better coder in the long run.\\n\\n5. Stay updated: The field of coding is constantly evolving, so it's important to stay updated on the latest technologies, tools, and best practices. Follow coding blogs, attend webinars, or take online courses to continue learning and growing as a coder.\\n\\n6. Seek feedback: Don't be afraid to ask for feedback on your code from more experienced developers or peers. Constructive feedback can help you identify areas for improvement and become a better coder.\\n\\n7. Set goals: Setting specific coding goals can help you stay motivated and focused on your coding journey. Whether it's completing a certain number of coding challenges or mastering a new programming language, setting goals can help you track your progress and improve your coding skills. \\n\\nRemember, becoming a better coder takes time and dedication, so be patient with yourself and keep practicing.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 15, 'total_tokens': 351, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-3165be73-63b2-4371-8604-098d143e5975-0', usage_metadata={'input_tokens': 15, 'output_tokens': 336, 'total_tokens': 351, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"how can I be better at coding ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a world class engineer with more than 20y of experience.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"To become better at coding, I would suggest the following strategies based on my experience as a world-class engineer:\\n\\n1. **Practice Regularly**: Like any skill, coding requires regular practice to improve. Code consistently, work on small projects, and challenge yourself with increasingly difficult tasks.\\n\\n2. **Learn from Others**: Collaborate with more experienced developers, participate in code reviews, and seek feedback on your code. This will help you learn new techniques and best practices.\\n\\n3. **Read Code**: Study code written by experienced developers. Reading and understanding high-quality code is a great way to learn new concepts and improve your coding skills.\\n\\n4. **Continuous Learning**: Stay updated on the latest technologies, tools, and industry trends. Attend workshops, conferences, and read books or online resources to expand your knowledge.\\n\\n5. **Write Clean and Maintainable Code**: Focus on writing code that is easy to read, maintain, and debug. Follow coding standards and best practices to ensure the quality of your codebase.\\n\\n6. **Problem Solving Skills**: Improve your problem-solving skills by practicing algorithms and data structures. Understanding fundamental concepts will not only help you write efficient code but also solve complex problems more effectively.\\n\\n7. **Debugging Skills**: Learn to debug effectively by understanding how to use debugging tools, log messages, and analyze code execution. Debugging is a crucial skill in becoming a proficient coder.\\n\\n8. **Code Reviews**: Participate in or conduct code reviews to learn from others and receive feedback on your own code. Code reviews help identify issues early and improve the quality of code.\\n\\n9. **Build Projects**: Build real-world projects to apply your coding skills in practical scenarios. This will give you hands-on experience and help you understand how different components work together.\\n\\n10. **Balance Speed and Quality**: While it's important to write code efficiently, always prioritize quality over speed. Rushing through coding tasks may lead to errors and technical debt in the long run.\\n\\nRemember, becoming better at coding is a continuous journey. Practice, patience, and a willingness to learn and improve are key to mastering your coding skills.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 424, 'prompt_tokens': 34, 'total_tokens': 458, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-94bf683d-f77d-4853-9ce8-ef42bf212b88-0', usage_metadata={'input_tokens': 34, 'output_tokens': 424, 'total_tokens': 458, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"how can I be better at coding ?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser \n",
    "\n",
    "LangChain's parsers are specialized tools that help extract structured data from unstructured text or LLM outputs. They convert raw text into usable formats like dictionaries, lists, or custom objects. Let's follow the doc and use `StrOutputParser` class \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To become better at coding, especially with over 20 years of experience, here are some advanced tips:\\n\\n1. **Continuous Learning**: Stay updated with the latest technologies, trends, and best practices in the industry. Attend workshops, webinars, and conferences to keep your skills sharp.\\n\\n2. **Practice Regularly**: Just like any skill, coding improves with practice. Challenge yourself with coding puzzles, open source projects, or personal projects that push your boundaries.\\n\\n3. **Code Reviews**: Engage in code reviews with colleagues or in online communities. This not only helps you learn new techniques but also provides valuable feedback on your own code.\\n\\n4. **Refactor and Optimize**: Take the time to refactor your code regularly. Optimize for performance and readability. This exercise will help you write cleaner, more efficient code.\\n\\n5. **Build Complex Projects**: Undertake challenging projects that require you to dive deep into complex problems. Solving these will broaden your knowledge and skill set.\\n\\n6. **Teach Others**: One of the best ways to solidify your knowledge is by teaching others. Conduct workshops, mentor junior developers, or write technical blogs to share your expertise.\\n\\n7. **Learn New Languages**: If you primarily code in one language, consider learning a new one. This will not only broaden your skill set but also introduce you to different paradigms and ways of thinking.\\n\\n8. **Version Control**: Make sure you are proficient in using version control systems like Git. Understanding branching, merging, and resolving conflicts is essential in collaborative coding environments.\\n\\n9. **Automate**: Look for opportunities to automate repetitive tasks in your coding workflow. This could be through scripts, tools, or even setting up continuous integration/continuous deployment pipelines.\\n\\n10. **Network**: Connect with other developers, participate in online coding communities, and share your knowledge. Engaging with a broader community can expose you to different perspectives and approaches to coding.\\n\\nRemember, becoming better at coding is a continuous journey. Stay curious, be open to learning, and always strive to improve your skills.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"how can I be better at coding ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.invoke(\"how can I be better at coding ?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will not be interacting with the [Retrieval Chain](https://python.langchain.com/v0.1/docs/get_started/quickstart/) since we do not need web informtions, but you can check it from the doc üòé\n",
    "\n",
    "In the next part let's see how to connect langchain and chroma [here](https://python.langchain.com/docs/integrations/vectorstores/chroma/#basic-initialization) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU chromadb langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your mini mission "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have the basics your mission is : code a function named batch_upload_document like below and upload this [PDF](https://arxiv.org/abs/1706.03762) inside your collection and do a simple query \n",
    "\n",
    "```python\n",
    "def batch_upload_documents(\n",
    "    files: List[UploadFile] = File(...),\n",
    "    title_prefix: str = Form(\"Document\"),\n",
    "    source: str = Form(\"\"),\n",
    "    author: str = Form(\"\"),\n",
    "    tags: str = Form(\"\"),\n",
    "    chunk_size: int = Form(1000, description=\"Size of text chunks for PDF documents\"),\n",
    "    chunk_overlap: int = Form(100, description=\"Overlap between text chunks for PDF documents\"),\n",
    "    index_id: Optional[str] = Form(None, description=\"Optional index ID to add documents to\")):\n",
    "    \"\"\"\n",
    "    Upload multiple documents to the knowledge base and optionally add to an index.\n",
    "    Handles both text files and PDFs.\n",
    "    \n",
    "    For PDF files, the document will be split into chunks using LangChain's text splitter.\n",
    "    \"\"\"\n",
    "    #TODO: code here \n",
    "    pass \n",
    "```\n",
    "\n",
    "You should see this kind of results below ü•∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ids': [['a39e9252-9f9d-4787-98af-e428a1a8088f',\n",
       "   'dcff76f7-450b-4315-a348-20368b50afcc',\n",
       "   'f9b79128-b5e9-4094-afc9-649096630d00',\n",
       "   'db5de2e2-4281-48d8-bc51-5d928e7c3720',\n",
       "   '70c60f56-8215-4e21-86be-e6e6ee061ea9']],\n",
       " 'distances': [[0.9198479056358337,\n",
       "   0.9198479056358337,\n",
       "   0.9216511249542236,\n",
       "   0.9216515948930766,\n",
       "   0.9234551787376404]],\n",
       " 'embeddings': None,\n",
       " 'metadatas': [[{'author': '',\n",
       "    'created_at': '2025-03-11T00:08:04.573912',\n",
       "    'description': 'PDF page 9 uploaded as part of batch on 2025-03-11',\n",
       "    'filename': 'attention.pdf_page9_chunk37',\n",
       "    'id': 'a39e9252-9f9d-4787-98af-e428a1a8088f',\n",
       "    'source': 'attention.pdf',\n",
       "    'tags': 'pdf',\n",
       "    'title': 'Document 1: attention.pdf (Page 9, Chunk 37)',\n",
       "    'updated_at': '2025-03-11T00:08:04.573923'},\n",
       "   {'author': '',\n",
       "    'created_at': '2025-03-11T00:07:29.513343',\n",
       "    'description': 'PDF page 9 uploaded as part of batch on 2025-03-11',\n",
       "    'filename': 'attention.pdf_page9_chunk37',\n",
       "    'id': 'dcff76f7-450b-4315-a348-20368b50afcc',\n",
       "    'source': 'attention.pdf',\n",
       "    'tags': 'pdf',\n",
       "    'title': 'Document 1: attention.pdf (Page 9, Chunk 37)',\n",
       "    'updated_at': '2025-03-11T00:07:29.513351'},\n",
       "   {'author': '',\n",
       "    'created_at': '2025-03-11T00:07:29.650269',\n",
       "    'description': 'PDF page 12 uploaded as part of batch on 2025-03-11',\n",
       "    'filename': 'attention.pdf_page12_chunk47',\n",
       "    'id': 'f9b79128-b5e9-4094-afc9-649096630d00',\n",
       "    'source': 'attention.pdf',\n",
       "    'tags': 'pdf',\n",
       "    'title': 'Document 1: attention.pdf (Page 12, Chunk 47)',\n",
       "    'updated_at': '2025-03-11T00:07:29.650277'},\n",
       "   {'author': '',\n",
       "    'created_at': '2025-03-11T00:08:04.703649',\n",
       "    'description': 'PDF page 12 uploaded as part of batch on 2025-03-11',\n",
       "    'filename': 'attention.pdf_page12_chunk47',\n",
       "    'id': 'db5de2e2-4281-48d8-bc51-5d928e7c3720',\n",
       "    'source': 'attention.pdf',\n",
       "    'tags': 'pdf',\n",
       "    'title': 'Document 1: attention.pdf (Page 12, Chunk 47)',\n",
       "    'updated_at': '2025-03-11T00:08:04.703657'},\n",
       "   {'author': '',\n",
       "    'created_at': '2025-03-11T00:07:29.350140',\n",
       "    'description': 'PDF page 6 uploaded as part of batch on 2025-03-11',\n",
       "    'filename': 'attention.pdf_page6_chunk24',\n",
       "    'id': '70c60f56-8215-4e21-86be-e6e6ee061ea9',\n",
       "    'source': 'attention.pdf',\n",
       "    'tags': 'pdf',\n",
       "    'title': 'Document 1: attention.pdf (Page 6, Chunk 24)',\n",
       "    'updated_at': '2025-03-11T00:07:29.350149'}]],\n",
       " 'documents': [['model outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.',\n",
       "   'model outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.',\n",
       "   'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13',\n",
       "   'Attention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‚Äòmaking‚Äô, completing the phrase ‚Äòmaking...more difficult‚Äô. Attentions here shown only for\\nthe word ‚Äòmaking‚Äô. Different colors represent different heads. Best viewed in color.\\n13',\n",
       "   'between any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k ¬∑ n ¬∑ d + n ¬∑ d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million']],\n",
       " 'uris': None,\n",
       " 'data': None,\n",
       " 'included': ['distances', 'documents', 'metadatas']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collection.query(\n",
    "    query_texts=[\"attention\"],\n",
    "    n_results=5\n",
    ")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmops",
   "language": "python",
   "name": "llmops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
